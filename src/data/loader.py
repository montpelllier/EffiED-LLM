"""
Dataset Loader
Responsible for loading various datasets from the datasets directory.
Based on the original detection.init Dataset class implementation.
"""
import warnings
from pathlib import Path
from typing import Dict, Any

import pandas as pd


class DatasetLoader:
    """
    Dataset loader for loading clean/dirty data and rule files.

    This class provides functionality to load datasets from a structured directory format
    where each dataset contains:
    - clean.csv: Clean reference data
    - dirty.csv: Data containing errors
    - rule.json: Data quality rules and constraints

    The loader automatically generates error labels by comparing clean and dirty data.
    Preprocessing is handled separately by the DataPreprocessor class.
    """

    def __init__(self, datasets_root: str = None):
        """
        Initialize the DatasetLoader.

        Args:
            datasets_root: Root directory path containing datasets. If None, defaults to
                          the 'datasets' folder in the project root directory.
        """
        if datasets_root is None:
            # Use the default datasets directory relative to this file
            root_dir = Path(__file__).parent.parent.parent
            self.datasets_root = root_dir / "datasets"
        else:
            self.datasets_root = Path(datasets_root)

    def get_available_datasets(self) -> list:
        """
        Get a list of available datasets in the datasets directory.

        Returns:
            List of dataset names (directory names) sorted alphabetically.
        """
        datasets = []
        if self.datasets_root.exists():
            for item in self.datasets_root.iterdir():
                if item.is_dir():
                    datasets.append(item.name)
        return sorted(datasets)

    def load_dataset(self, dataset_name: str) -> Dict[str, Any]:
        """
        Load a complete dataset including clean data, dirty data, rules, and error labels.

        This method loads all components of a dataset and automatically generates error labels
        by comparing clean and dirty data. No preprocessing is applied at this stage.

        Args:
            dataset_name: Name of the dataset to load (must match directory name)

        Returns:
            Dictionary containing:
                - 'clean_data': DataFrame of clean reference data
                - 'dirty_data': DataFrame of data containing errors
                - 'rules': Dictionary of data quality rules processed from JSON
                - 'error_labels': Boolean DataFrame indicating error locations

        Raises:
            FileNotFoundError: If the dataset directory does not exist

        Note:
            - Data is loaded with dtype=str to preserve original formatting
            - Error labels are generated by element-wise comparison of clean vs dirty data
            - Rules are processed from JSON format to indexed dictionary format
            - For preprocessing, use DataPreprocessor class separately
        """
        dataset_path = self.datasets_root / dataset_name

        if not dataset_path.exists():
            raise FileNotFoundError(f"Dataset path does not exist: {dataset_path}")

        result = {}

        # Load clean and dirty data using original read parameters
        clean_path = dataset_path / "clean.csv"
        dirty_path = dataset_path / "dirty.csv"
        if clean_path.exists():
            result['clean_data'] = pd.read_csv(clean_path, dtype=str, na_values=[], keep_default_na=False)
        else:
            warnings.warn(f"Clean data file does not exist: {clean_path}")
            result['clean_data'] = None

        if dirty_path.exists():
            result['dirty_data'] = pd.read_csv(dirty_path, dtype=str, na_values=[], keep_default_na=False)
            # Generate error labels using original implementation logic
            # result['error_labels'] = result['dirty_data'].ne(result['clean_data'])
        else:
            raise ValueError(f"Dirty data file does not exist: {dirty_path}")

        # Load rule file using original implementation logic
        rules_file = dataset_path / "rule.json"
        if rules_file.exists():
            rules_df = pd.read_json(rules_file)
            columns_df = pd.json_normalize(rules_df['columns'].tolist())
            result['rules'] = columns_df.set_index('name').to_dict(orient='index')
        else:
            warnings.warn(f"Rules file does not exist: {rules_file}")
            result['rules'] = None

        return result

    def load_clean_data(self, dataset_name: str) -> pd.DataFrame:
        """
        Load only the clean data for a specified dataset.

        Args:
            dataset_name: Name of the dataset to load

        Returns:
            DataFrame containing clean reference data
        """
        dataset = self.load_dataset(dataset_name)
        return dataset['clean_data']

    def load_dirty_data(self, dataset_name: str) -> pd.DataFrame:
        """
        Load only the dirty data for a specified dataset.

        Args:
            dataset_name: Name of the dataset to load

        Returns:
            DataFrame containing data with errors
        """
        dataset = self.load_dataset(dataset_name)
        return dataset['dirty_data']

    def load_rules(self, dataset_name: str) -> Dict[str, Any]:
        """
        Load only the data quality rules for a specified dataset.

        Args:
            dataset_name: Name of the dataset to load

        Returns:
            Dictionary containing processed rules indexed by column name
        """
        dataset = self.load_dataset(dataset_name)
        return dataset['rules']

    def get_dataset_info(self, dataset_name: str) -> Dict[str, Any]:
        """
        Get comprehensive information about a dataset without loading all data.

        This method provides metadata and statistics about the dataset including
        data availability, shapes, column information, and error statistics.

        Args:
            dataset_name: Name of the dataset to analyze

        Returns:
            Dictionary containing dataset information:
                - 'name': Dataset name
                - 'has_clean_data': Whether clean data is available
                - 'has_dirty_data': Whether dirty data is available
                - 'has_rules': Whether rules are available
                - 'clean_shape': Shape of clean data (if available)
                - 'dirty_shape': Shape of dirty data (if available)
                - 'columns': List of column names (if data available)
                - 'num_rules': Number of rules (if rules available)
        """
        dataset = self.load_dataset(dataset_name)

        info = {
            'name': dataset_name,
            'has_clean_data': dataset['clean_data'] is not None,
            'has_dirty_data': dataset['dirty_data'] is not None,
            'has_rules': dataset['rules'] is not None,
        }

        if dataset['clean_data'] is not None:
            info['clean_shape'] = dataset['clean_data'].shape
            info['columns'] = list(dataset['clean_data'].columns)

        if dataset['dirty_data'] is not None:
            info['dirty_shape'] = dataset['dirty_data'].shape

        if dataset['rules'] is not None:
            info['num_rules'] = len(dataset['rules'])

        return info
